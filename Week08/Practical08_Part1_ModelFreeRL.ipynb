{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a0d1006",
   "metadata": {},
   "source": [
    "# Practical08_Part1_ModelFreeRL.ipynb\n",
    "\n",
    "### Import required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e221078",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "734cec78",
   "metadata": {},
   "source": [
    "# 1. The Grid World environment\n",
    "\n",
    "Recall the grid in which our robot lives\n",
    "\n",
    "![GridWorldExample.png](https://i.postimg.cc/5tMM5vqf/Grid-World-Example.png)\n",
    "\n",
    "- The states $s \\in \\mathcal{S}$ correspond to locations in the grid. Each location has also a cell index associated to it, e.g., cell index 4 is associated to location (row=1,col=0)\n",
    "- The robot can move up, down, left, or right. Actions correpond to unit increments or decrements in the specified direction.\n",
    "    - Up : (-1,0)\n",
    "    - Down: (1,0)\n",
    "    - Left: (0,-1)\n",
    "    - Right: (0, 1)\n",
    "- Each action is represented by a number. Action (Up) is represented by 0, (Down) by 1, (Left) by 2 and, finally, (Right) by 3. No actions are available at a terminal state\n",
    "- Discount factor $\\gamma = 0.99$ (class attribute ``gamma=0.99``)\n",
    "- Stochastic transition matrix (class attribute ``noise=0.2``)\n",
    "- Rewards are only obtained at terminal states (class attribute ``living_reward=-0.04``)\n",
    "\n",
    "This environment is represented with the class ``GridEnv``. **To a look at the attributes of this class, place the cursor somewhere on the class' name and hit SHIFT+TAB (local Jupyter Notebook) or hover your mouse over it (Colab). If there's a + button at the top of the popup tooltip, this means the documentation spans a few lines, click it to show the full docstring, then scroll up.**\n",
    "\n",
    "### Known Model\n",
    "\n",
    "Recall also the **optimal policy** we found yesterday using policy-interation\n",
    "\n",
    "![example_policy.png](https://i.postimg.cc/pLjHnkj0/example-policy.png)\n",
    "\n",
    "since the dynamics of our grid world environment are known, we obtained the state-value function $v_\\pi(s)$ associated to this policy using ``policy_evalution(.)`` \n",
    "\n",
    "We have defined the class ``GridEnv`` to represent our Grid World MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7b874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Grid World instance\n",
    "grid_world = GridEnv(gamma=0.99, noise=0.2, living_reward=-0.04)\n",
    "\n",
    "# Get policy shown in image\n",
    "policy_pi = encode_policy(grid_world)\n",
    "\n",
    "# Compute value-function using dynamic programming\n",
    "v_pi = policy_evaluation(grid_world, policy_pi)\n",
    "\n",
    "plot_value_function(grid_world, v_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a3ead8",
   "metadata": {},
   "source": [
    "# 2. Policy Evaluation\n",
    "\n",
    "## 2.1 Monte-Carlo Method\n",
    "\n",
    "### What if the Transition and Reward Function are Unknown?\n",
    "\n",
    "Let's first define the helper method ``generate_episode(.)``. It samples an episode i.e., a sequence of ($s, a, r, s'$) tuples, from a given policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f98575",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "780601e2",
   "metadata": {},
   "source": [
    "Now, under the assumption that $\\mathcal{T}(s,a,s')$ and $\\mathcal{R}(s,a)$ are unknown, let's use the algorithm shown below to get an estimate $\\hat{v}_\\pi(s)$ of the true state-value function $v_\\pi(s)$\n",
    "\n",
    "![MCPolicyEvaluation.png](https://i.postimg.cc/6pXj5P6D/MCPolicy-Evaluation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4515864",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c8881ce1",
   "metadata": {},
   "source": [
    "Let's now try the algorithm and compare its out to the true value-state function. \n",
    "\n",
    "**Interaction**:\n",
    "Run the algorithm multiple times and observe what happens when the number of episodes increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8ed34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change n_episodes to see what happens\n",
    "errors, predicted_v = monte_carlo_first_visit_policy_evaluation(grid_world, policy_pi, v_pi, n_episodes=100)\n",
    "\n",
    "fig = plt.figure(constrained_layout=True)\n",
    "spec = gridspec.GridSpec(ncols=2, nrows=2, figure=fig)\n",
    "f_ax1 = fig.add_subplot(spec[0, 0])\n",
    "f_ax2 = fig.add_subplot(spec[0, 1])\n",
    "f_ax3 = fig.add_subplot(spec[1, :])\n",
    "\n",
    "#Plot true value function\n",
    "plot_value_function(grid_world, v_pi, f_ax1)\n",
    "f_ax1.set_title(\"True state-value function\")\n",
    "\n",
    "plot_value_function(grid_world, predicted_v, f_ax2)\n",
    "f_ax2.set_title(\"Predicted state-value function\")\n",
    "\n",
    "f_ax3.plot(errors)\n",
    "f_ax3.set_title(\"Predicted Error (sum of abs. differences)\")\n",
    "f_ax3.set_xlabel(\"Num. episodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e00dd3",
   "metadata": {},
   "source": [
    "# 2.2. Temporal Difference Method\n",
    "Estimate $\\hat{v}_\\pi(s)$ of the true state-value function $v_\\pi(s)$\n",
    "\n",
    "![TDPolicyEvaluation.png](https://i.postimg.cc/c4yywX4c/TDPolicy-Evaluation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6baaaa",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "01fbfe06",
   "metadata": {},
   "source": [
    "Let's now try the algorithm and compare its output to the true value-state function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f223b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change n_episodes to see what happens\n",
    "errors, predicted_v = temporal_learning_policy_evaluation(grid_world, policy_pi, v_pi, alpha=0.1, n_episodes=200)\n",
    "\n",
    "fig = plt.figure(constrained_layout=True)\n",
    "spec = gridspec.GridSpec(ncols=2, nrows=2, figure=fig)\n",
    "f_ax1 = fig.add_subplot(spec[0, 0])\n",
    "f_ax2 = fig.add_subplot(spec[0, 1])\n",
    "f_ax3 = fig.add_subplot(spec[1, :])\n",
    "\n",
    "#Plot true value function\n",
    "plot_value_function(grid_world, v_pi, f_ax1)\n",
    "f_ax1.set_title(\"True state-value function\")\n",
    "\n",
    "plot_value_function(grid_world, predicted_v, f_ax2)\n",
    "f_ax2.set_title(\"Predicted state-value function\")\n",
    "\n",
    "f_ax3.plot(errors)\n",
    "f_ax3.set_title(\"Predicted Error (sum of abs. differences)\")\n",
    "f_ax3.set_xlabel(\"Num. episodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8395007",
   "metadata": {},
   "source": [
    "# 3. Control\n",
    "\n",
    "In a model-free setting where our state-value and action-value estimates depend on the actions chosen by the agent, how can we guarantee that the all actions will continue to be selected?\n",
    "\n",
    "## 3.1 $\\epsilon$-Greedy Policies\n",
    "\n",
    "We can use an $\\epsilon$-greedy policy. This type of policy are formally defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "    \\pi(a|s) = \n",
    "    \\begin{cases}\n",
    "        1 - \\epsilon + \\frac{\\epsilon}{|\\mathcal{A}|},&  \\text{if } a^* = \\arg\\max_{a \\in \\mathcal{A}} q_\\pi(s,a)\\\\\n",
    "        \\frac{\\epsilon}{|\\mathcal{A}|}, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "Let's see how the agent behaves when it follows an $\\epsilon$-greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d007da3",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318da9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set noise to zero. Randomness in agent behaviour is only due to e-greedy policy\n",
    "grid_world = GridEnv(noise=0, living_reward=-0.04, gamma=0.99)\n",
    "\n",
    "# Get policy shown in section 1\n",
    "policy_pi = encode_policy(grid_world)\n",
    "\n",
    "# Compute value-function using dynamic programming\n",
    "v_pi = policy_evaluation(grid_world, policy_pi)\n",
    "\n",
    "# Use value-function to compute q-values\n",
    "q_pi = grid_world.get_q_values(v_pi)\n",
    "\n",
    "# Start episode\n",
    "cur_state = grid_world.idx_cur_state\n",
    "s_x, s_y = get_state_to_plot(grid_world)\n",
    "\n",
    "# We can visualize our grid world using the render() function\n",
    "fig, ax = grid_world.render()\n",
    "agent, = ax.plot([], [], 'o', color='b', linewidth=6)\n",
    "reward_text = ax.text(0.02, 0.95, '', transform=ax.transAxes)\n",
    "\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "path_to_plot = []\n",
    "\n",
    "v_epsilon = 0.8\n",
    "\n",
    "while not done:\n",
    "    action = get_egreedy_action(grid_world, cur_state, q_pi, v_epsilon)\n",
    "    cur_state, cur_reward, done, _ = grid_world.step(int(action))\n",
    "    n_x, n_y = get_state_to_plot(grid_world)\n",
    "    cumulative_reward += cur_reward\n",
    "    path_to_plot.append([cumulative_reward, n_x, n_y])\n",
    "\n",
    "def init():\n",
    "    agent.set_data(s_x + 0.5, s_y + 0.5)\n",
    "    reward_text.set_text('')\n",
    "    return agent, reward_text\n",
    "\n",
    "def animate(i):\n",
    "    if i < len(path_to_plot):\n",
    "        r, n_x, n_y = path_to_plot[i]\n",
    "        agent.set_data(n_x + 0.5, n_y + 0.5)\n",
    "        reward_text.set_text('Cumulative reward: %.2f' % r)\n",
    "    return agent, reward_text\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(path_to_plot), blit=False, interval=500, init_func=init,\n",
    "                              repeat=False)\n",
    "\n",
    "plt.close('all') \n",
    "display(HTML(f\"<div align=\\\"center\\\">{ani.to_jshtml()}</div>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edc6ff8",
   "metadata": {},
   "source": [
    "## 3.2 Q-Learning\n",
    "\n",
    "We have seen how to evaluate a policy without a model. Let's now find an *approximately* optimal policy using the off-policy control method Q-learning.\n",
    "\n",
    "To help during the learning, we have added a lambda function that iteratively decreases epsilon. Our agent will strongly explore the environment at first to then swicth into exploitation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086f0508",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cf404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "ax.plot([epsilon_by_episode(i) for i in range(500)])\n",
    "ax.set_xlabel(\"Num. episodes\")\n",
    "ax.set_ylabel(\"Epsilon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaca774b",
   "metadata": {},
   "source": [
    "Here is our implementation of the q-learning algorithm shown below\n",
    "\n",
    "![q-learning.png](https://i.postimg.cc/8z70Yv5C/q-learning.png)\n",
    "\n",
    "**Complete the missing steps**:\n",
    "- Choose an action using an $\\epsilon$-greedy policy (use the function ``get_egreedy_action(.)`` we tested in Section 3.1)\n",
    "- Update our q-function using a greedy (max) policy (use ``q_function[cur_state][action]`` to index our q-function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cbcad0",
   "metadata": {},
   "source": [
    "**Keep in Mind**: Correspondance between the mathematical notation and implemented code\n",
    "\n",
    "|                 |                            |                  |\n",
    "| :-------------- | -------------------------: | ---------------: |\n",
    "|                 | **Variable/Attribute**     | **Type**         | \n",
    "| $\\epsilon$      | `epsilon_by_episode`       | `float`          |\n",
    "| $\\alpha$        | `alpha`                    | `float`          | \n",
    "| $\\gamma$        | `grid_world.gamma`         | `float`          | \n",
    "| $\\hat{q}(s, a)$ | `q_function[idx_s][idx_a]` | `dict` of `dict` | \n",
    "| $s$             | `cur_state`                | `int`            | \n",
    "| $r$             | `reward`                   | `int`            |\n",
    "| $s^{\\prime}$    | `next_state`               | `int`            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b53db8",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "29258a02",
   "metadata": {},
   "source": [
    "Let's now test our implementation and compare our free-model policy with the one we obtained in the last lecture using value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda29396",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "cell_id": "0cc61a62f28b4f72a055fd365ed3983d",
    "tags": [],
    "deepnote_cell_type": "code",
    "deepnote_cell_height": 61
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=d30c9d78-b335-4e3c-b9dd-5dae33556494' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "deepnote_notebook_id": "697e8118-f25b-4205-9543-ecd4fe01e8d6",
  "deepnote": {},
  "deepnote_execution_queue": []
 }
}